{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b07305bb-cab7-49b5-a52b-e548c04151ee",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "cab04931-07aa-4159-981e-c0d2a470bb45",
   "metadata": {},
   "source": [
    "## Process and Recognize Speech:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5a500433-7416-457c-8a90-904486fd13f3",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'librosa'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mtf\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mlibrosa\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpyplot\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mplt\u001b[39;00m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'librosa'"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import librosa\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "# Load and preprocess an audio file\n",
    "def preprocess_audio(file_path):\n",
    "    audio, sr = librosa.load(file_path, sr=16000)  # Load audio with a sample rate of 16000 Hz\n",
    "    mel_spectrogram = librosa.feature.melspectrogram(y=audio, sr=sr, n_mels=64, fmax=8000)\n",
    "    mel_spectrogram = librosa.power_to_db(mel_spectrogram, ref=np.max)  # Convert to dB scale\n",
    "    return mel_spectrogram\n",
    "\n",
    "# Simple CNN-RNN Model for speech recognition\n",
    "def build_model(input_shape):\n",
    "    model = tf.keras.Sequential([\n",
    "        # CNN for feature extraction\n",
    "        layers.InputLayer(input_shape=input_shape),\n",
    "        layers.Conv2D(32, kernel_size=(3, 3), activation='relu'),\n",
    "        layers.MaxPooling2D(pool_size=(2, 2)),\n",
    "        layers.Conv2D(64, kernel_size=(3, 3), activation='relu'),\n",
    "        layers.MaxPooling2D(pool_size=(2, 2)),\n",
    "        \n",
    "        # Reshaping to fit RNN\n",
    "        layers.Reshape(target_shape=(-1, 64)),  # Flatten before RNN\n",
    "        \n",
    "        # RNN layer for sequential learning\n",
    "        layers.LSTM(128, return_sequences=True),\n",
    "        layers.LSTM(128),\n",
    "        \n",
    "        # Dense layer for output (e.g., predicting text)\n",
    "        layers.Dense(128, activation='relu'),\n",
    "        layers.Dense(26, activation='softmax')  # Assuming 26 possible characters (A-Z)\n",
    "    ])\n",
    "    return model\n",
    "\n",
    "# Example usage: Preprocess an audio file and feed it into the model\n",
    "audio_file = \"path_to_audio.wav\"  # Replace with an actual file path\n",
    "mel_spectrogram = preprocess_audio(audio_file)\n",
    "\n",
    "# Build the model\n",
    "input_shape = mel_spectrogram.shape + (1,)  # Add channel dimension\n",
    "model = build_model(input_shape)\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Visualize the spectrogram\n",
    "plt.imshow(mel_spectrogram, cmap='viridis')\n",
    "plt.title(\"Mel Spectrogram\")\n",
    "plt.show()\n",
    "\n",
    "# You would now proceed with training the model on a dataset of labeled speech audio\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2688dfd-0f1d-4f32-9c14-1f13640232ee",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
